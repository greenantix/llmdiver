This file is a merged representation of a subset of the codebase, containing specifically included files and files not matching ignore patterns, combined into a single document by Repomix.
The content has been processed where comments have been removed, empty lines have been removed, content has been compressed (code blocks are separated by ⋮---- delimiter).

# File Summary

## Purpose
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

## File Format
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  a. A header with the file path (## File: path/to/file)
  b. The full contents of the file in a code block

## Usage Guidelines
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

## Notes
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: *.py, *.js, *.ts, *.jsx, *.tsx, *.sh, *.rs, *.go, *.java, *.c, *.cpp, *.h, Dockerfile, docker-compose.yml, *.yaml, *.yml, *.toml, *.ini, *.conf
- Files matching these patterns are excluded: *.md, *.log, *.tmp, *.cache, *.bak, *.swp, *.swo, node_modules, __pycache__, .git, .llmdiver, venv, .venv, env, .env, dist, build, target, coverage, .coverage, .pytest_cache, *.min.js, *.bundle.js, *.test.js, *.spec.js, *test*.py, *spec*.py, migrations, fixtures, mock*, test_data, sample_data, *.mock.*
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Code comments have been removed from supported file types
- Empty lines have been removed from all files
- Content has been compressed - code blocks are separated by ⋮---- delimiter
- Files are sorted by Git change count (files with more changes are at the bottom)

# Directory Structure
```
audit.sh
claude_sync_template.sh
decorators.py
easy_install.sh
easy_monitor.sh
install.sh
llmdiver_control.sh
llmdiver_daemon.py
llmdiver_gui.py
llmdiver_monitor.py
llmdiver-daemon.py
run_llm_audit.sh
send_to_claude.sh
setup_env.sh
start_llmdiver.sh
start_with_audit.sh
start-llmdiver.sh
test_llmdiver.sh
watch_and_audit.sh
```

# Files

## File: audit.sh
````bash
SCRIPT_DIR="$(dirname "$0")"
PROJECT_ROOT="$SCRIPT_DIR"
LM_URL="http://localhost:1234"
TIMEOUT=600
while [[ $
    case $1 in
        --timeout)
            TIMEOUT="$2"
            shift 2
            ;;
        --lm-url)
            LM_URL="$2"
            shift 2
            ;;
        *)
            echo "Unknown option: $1"
            echo "Usage: $0 [--timeout SECONDS] [--lm-url URL]"
            exit 1
            ;;
    esac
done
echo "🔍 Checking LM Studio connection..."
curl -s "$LM_URL/v1/models" > /dev/null
if [ $? -ne 0 ]; then
    echo "❌ Error: LM Studio server is not running at $LM_URL"
    echo "Please start LM Studio and ensure a model is loaded"
    echo "Or specify a different URL with --lm-url"
    exit 1
fi
echo "Settings:"
echo "• LM Studio URL: $LM_URL"
echo "• Analysis timeout: ${TIMEOUT}s"
echo "🔍 Starting comprehensive project audit using LM Studio..."
echo "This will perform a deep analysis of the codebase using AI."
echo "The analysis may take several minutes depending on the model and codebase size."
mkdir -p "$PROJECT_ROOT/audits"
echo -e "\n📋 Running deep code analysis..."
timeout ${TIMEOUT} python3 "$PROJECT_ROOT/tools/deep_audit.py" \
    "$PROJECT_ROOT" \
    --timeout "$TIMEOUT" \
    --lm-url "$LM_URL" &
PID=$!
DOTS=0
while kill -0 $PID 2>/dev/null; do
    echo -n "."
    DOTS=$((DOTS + 1))
    if [ $DOTS -eq 50 ]; then
        echo
        DOTS=0
    fi
    sleep 1
done
wait $PID
EXIT_CODE=$?
if [ $EXIT_CODE -eq 124 ]; then
    echo -e "\n❌ Analysis timed out after ${TIMEOUT} seconds!"
    echo "Try increasing timeout with --timeout option"
    exit 1
fi
if [ $? -ne 0 ]; then
    echo -e "\n❌ Analysis failed! Check the error messages above."
    exit 1
fi
cat > "$PROJECT_ROOT/audits/executive_summary.md" << EOL
Generated on: $(date "+%Y-%m-%d %H:%M:%S")
This is an automated summary of the comprehensive project analysis. The full detailed report can be found in \`audits/deep_analysis.md\`.
$(grep -A 5 "^## Layer" "$PROJECT_ROOT/audits/deep_analysis.md" | sed 's/^/- /')
$(grep -A 3 "### Security Concerns\|### Performance Issues" "$PROJECT_ROOT/audits/deep_analysis.md" | sed 's/^/- /')
1. Review and address all security concerns identified in the full report
2. Optimize performance bottlenecks highlighted in the analysis
3. Consider implementing suggested architectural improvements
4. Update documentation based on the current codebase structure
For detailed findings and recommendations, please review the complete analysis in \`audits/deep_analysis.md\`.
EOL
echo -e "\n📑 Generating final reports..."
if [ ! -f "$PROJECT_ROOT/audits/deep_analysis.md" ]; then
    echo -e "\n❌ Analysis failed to generate reports!"
    exit 1
fi
echo -e "\n✅ Analysis complete!"
echo -e "\nReport generated: audits/deep_analysis.md"
echo -e "\nThe report includes:"
echo "• Executive Summary of findings"
echo "• Detailed analysis of project structure"
echo "• Core functionality assessment"
echo "• Code quality metrics"
echo "• Security & performance insights"
echo "• Actionable recommendations"
echo -e "\nReview the report for comprehensive findings and improvement suggestions."
````

## File: claude_sync_template.sh
````bash
set -e
REPO_DIR="$(pwd)"
PROJECT_NAME="$(basename "$REPO_DIR")"
AUDIT_DIR="./audits/$PROJECT_NAME"
mkdir -p "$AUDIT_DIR"
echo "🔍 Starting LLMdiver audit for $PROJECT_NAME"
llm-audit-quick
echo "✅ Audit complete for $PROJECT_NAME"
echo "📁 Results saved in $AUDIT_DIR"
````

## File: decorators.py
````python
def analyze_source_decorators(source_lines)
⋮----
decorators = []
⋮----
line = line.strip()
⋮----
def count_decorators(decorators)
⋮----
counts = {}
⋮----
def parse_decorator_args(decorator_line)
⋮----
args_str = decorator_line.split('(', 1)[1].rsplit(')', 1)[0]
````

## File: easy_install.sh
````bash
set -e
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
TARGET_PROJECT_DIR="$(pwd)"
TARGET_BIN_DIR="$TARGET_PROJECT_DIR/node_modules/.bin"
if ! command -v jq &> /dev/null; then
    echo "❌ Error: jq is required but not installed"
    echo "Please install jq first:"
    echo "  Ubuntu/Debian: sudo apt-get install jq"
    echo "  MacOS: brew install jq"
    exit 1
fi
mkdir -p "$TARGET_BIN_DIR"
mkdir -p "$TARGET_PROJECT_DIR/audits"
LLMDIVER_DIR="$SCRIPT_DIR"
AUDIT_MAIN="$LLMDIVER_DIR/run_llm_audit.sh"
AUDIT_QUICK="$LLMDIVER_DIR/audit.sh"
CLAUDE_SYNC="$LLMDIVER_DIR/claude_sync_template.sh"
for file in "$AUDIT_MAIN" "$AUDIT_QUICK" "$CLAUDE_SYNC"; do
    if [[ ! -f "$file" ]]; then
        echo "❌ Error: Required file not found: $file"
        exit 1
    fi
done
ln -sf "$AUDIT_MAIN" "$TARGET_BIN_DIR/llm-audit"
ln -sf "$AUDIT_QUICK" "$TARGET_BIN_DIR/llm-audit-quick"
ln -sf "$CLAUDE_SYNC" "$TARGET_BIN_DIR/claude-sync"
chmod +x "$AUDIT_MAIN" "$AUDIT_QUICK" "$CLAUDE_SYNC"
CONFIG_DIR="$TARGET_PROJECT_DIR/config"
mkdir -p "$CONFIG_DIR"
if [[ ! -f "$CONFIG_DIR/llmdiver.json" ]]; then
    cat > "$CONFIG_DIR/llmdiver.json" << EOF
{
    "llm_model": "meta-llama-3.1-8b-instruct",
    "llm_temp": 0.3,
    "llm_url": "http://127.0.0.1:1234/v1/chat/completions"
}
EOF
fi
echo "✅ LLMdiver installed successfully!"
echo "Available commands:"
echo "  - llm-audit         (full deep scan)"
echo "  - llm-audit-quick   (smart fast audit)"
echo "  - claude-sync       (full auto Claude loop)"
echo ""
echo "Configuration:"
echo "  - Config file: $CONFIG_DIR/llmdiver.json"
echo "  - Audit results will be saved in: $TARGET_PROJECT_DIR/audits"
echo ""
echo "Make sure LM Studio is running before using the audit commands!"
````

## File: easy_monitor.sh
````bash
echo "Checking and installing required packages..."
check_package() {
    python3 -c "import $1" 2>/dev/null
    return $?
}
if ! command -v python3-tk &> /dev/null; then
    echo "Installing tkinter..."
    sudo apt-get update
    sudo apt-get install -y python3-tk
fi
REQUIRED_PACKAGES=("psutil" "watchdog" "GitPython" "scikit-learn" "numpy" "requests" "decorators")
for package in "${REQUIRED_PACKAGES[@]}"; do
    if ! check_package $package; then
        echo "Installing $package..."
        pip3 install $package
    fi
done
echo "Starting LLMdiver Monitor..."
export PYTHONPATH="${PYTHONPATH:+${PYTHONPATH}:}$(pwd)"
python3 ./llmdiver_monitor.py
````

## File: install.sh
````bash
set -e
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
TARGET_DIR="$SCRIPT_DIR/node_modules/.bin"
mkdir -p "$TARGET_DIR"
AUDIT_MAIN="$SCRIPT_DIR/run_llm_audit.sh"
AUDIT_QUICK="$SCRIPT_DIR/audit.sh"
CLAUDE_SYNC="$SCRIPT_DIR/claude_sync_template.sh"
if [[ ! -f "$AUDIT_MAIN" ]]; then
    AUDIT_MAIN="$(realpath "$SCRIPT_DIR/../LLMdiver/run_llm_audit.sh")"
    AUDIT_QUICK="$(realpath "$SCRIPT_DIR/../LLMdiver/audit.sh")"
    CLAUDE_SYNC="$(realpath "$SCRIPT_DIR/../LLMdiver/claude_sync_template.sh")"
fi
# Create symlinks
ln -sf "$AUDIT_MAIN" "$TARGET_DIR/llm-audit"
ln -sf "$AUDIT_QUICK" "$TARGET_DIR/llm-audit-quick"
ln -sf "$CLAUDE_SYNC" "$TARGET_DIR/claude-sync"
chmod +x "$AUDIT_MAIN" "$AUDIT_QUICK" "$CLAUDE_SYNC"
echo "✅ LLMdiver installed into $TARGET_DIR"
echo "Available commands:"
echo "  - llm-audit         (full deep scan)"
echo "  - llm-audit-quick   (smart fast audit)"
echo "  - claude-sync       (full auto Claude loop)"
````

## File: llmdiver_control.sh
````bash
echo "🚀 Starting LLMdiver Control Panel..."
if ! python3 -c "import tkinter" 2>/dev/null; then
    echo "❌ Error: Python tkinter not found!"
    echo "Install with: sudo apt-get install python3-tk"
    exit 1
fi
if [[ ! -f "start_llmdiver.sh" ]]; then
    echo "❌ Error: Please run from LLMdiver directory!"
    exit 1
fi
python3 llmdiver_gui.py
````

## File: llmdiver_gui.py
````python
class LLMdiverGUI
⋮----
def __init__(self, root)
def setup_ui(self)
⋮----
main_frame = ttk.Frame(self.root, padding="10")
⋮----
title_label = ttk.Label(main_frame, text="🤖 LLMdiver Control Panel",
⋮----
status_frame = ttk.LabelFrame(main_frame, text="Daemon Status", padding="10")
⋮----
control_frame = ttk.LabelFrame(main_frame, text="Daemon Control", padding="10")
⋮----
refresh_btn = ttk.Button(control_frame, text="🔍 Refresh Status", command=self.check_status)
⋮----
projects_frame = ttk.LabelFrame(main_frame, text="Monitored Projects", padding="10")
⋮----
projects_scroll = ttk.Scrollbar(projects_frame, orient=tk.VERTICAL, command=self.projects_tree.yview)
⋮----
log_frame = ttk.LabelFrame(main_frame, text="Live Logs", padding="10")
⋮----
log_controls = ttk.Frame(log_frame)
⋮----
clear_btn = ttk.Button(log_controls, text="Clear Logs", command=self.clear_logs)
⋮----
auto_scroll_cb = ttk.Checkbutton(log_controls, text="Auto-scroll",
⋮----
def run_command(self, command)
⋮----
result = subprocess.run(command, shell=True, capture_output=True, text=True, cwd=".")
⋮----
def check_status(self)
⋮----
lines = stdout.split('\n')
⋮----
pid = line.split("PID:")[-1].strip().rstrip(")")
⋮----
def start_daemon(self)
def stop_daemon(self)
def restart_daemon(self)
def pause_daemon(self)
⋮----
pid = f.read().strip()
⋮----
def resume_daemon(self)
def update_projects_list(self)
⋮----
config = json.load(f)
⋮----
repositories = config.get("repositories", [])
⋮----
name = repo.get("name", "Unknown")
path = repo.get("path", "Unknown")
auto_commit = "✅" if repo.get("auto_commit", False) else "❌"
discovered = " 🔍" if repo.get("discovered", False) else ""
⋮----
def start_log_monitoring(self)
def monitor_logs(self)
⋮----
log_file = "llmdiver_daemon.log"
⋮----
line = f.readline()
⋮----
def process_log_queue(self)
⋮----
line = self.log_queue.get_nowait()
⋮----
def auto_scroll(self)
def clear_logs(self)
def on_closing(self)
def main()
⋮----
root = tk.Tk()
app = LLMdiverGUI(root)
````

## File: send_to_claude.sh
````bash
PROJECT_NAME="GMAILspambot"
AUDIT_ROOT="/home/greenantix/AI/audits/$PROJECT_NAME"
PROMPTS_DIR="$AUDIT_ROOT/prompts"
OUTPUT_DIR="$AUDIT_ROOT/responses"
mkdir -p "$OUTPUT_DIR"
echo "🚀 Dispatching Claude tasks via CLI for $PROJECT_NAME..."
find "$PROMPTS_DIR" -type f -name 'CLAUDE_*.txt' | sort | while read -r PROMPT_FILE; do
    TASK_NAME=$(basename "$PROMPT_FILE" .txt | sed 's/^CLAUDE_//')
    OUTPUT_FILE="$OUTPUT_DIR/$TASK_NAME.response.md"
    echo "🧠 Task: $TASK_NAME"
    claude --dangerously-skip-permissions --continue <<EOF > "$OUTPUT_FILE"
You are Claude Code. Analyze the following task file and provide a complete implementation plan or code as needed.
=== TASK BEGIN ===
$(cat "$PROMPT_FILE")
=== TASK END ===
EOF
    echo "✅ Saved: $OUTPUT_FILE"
done
echo "📬 All tasks complete. Responses saved in: $OUTPUT_DIR"
````

## File: setup_env.sh
````bash
export PATH="$HOME/.local/bin:$PATH"
````

## File: start_llmdiver.sh
````bash
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
cd "$SCRIPT_DIR"
DAEMON_SCRIPT="llmdiver_daemon.py"
PID_FILE="llmdiver.pid"
LOG_FILE="llmdiver_daemon.log"
start_daemon() {
    if [ -f "$PID_FILE" ]; then
        PID=$(cat "$PID_FILE")
        if ps -p "$PID" > /dev/null 2>&1; then
            echo "LLMdiver daemon is already running (PID: $PID)"
            return 1
        else
            echo "Removing stale PID file"
            rm -f "$PID_FILE"
        fi
    fi
    echo "Starting LLMdiver daemon..."
    if ! command -v repomix &> /dev/null; then
        echo "❌ Error: repomix not found. Install with: npm install -g repomix"
        exit 1
    fi
    if ! python3 -c "import git, watchdog" 2>/dev/null; then
        echo "❌ Error: Missing Python dependencies. Install with: pip install gitpython watchdog"
        exit 1
    fi
    if ! curl -s http://127.0.0.1:1234/v1/models >/dev/null 2>&1; then
        echo "⚠️  Warning: LM Studio not accessible at http://127.0.0.1:1234"
        echo "   Make sure LM Studio is running with API enabled"
    fi
    mkdir -p config
    nohup python3 "$DAEMON_SCRIPT" > "$LOG_FILE" 2>&1 &
    PID=$!
    echo $PID > "$PID_FILE"
    sleep 2
    if ps -p "$PID" > /dev/null 2>&1; then
        echo "✅ LLMdiver daemon started successfully (PID: $PID)"
        echo "📋 Log file: $LOG_FILE"
        echo "🔍 Monitor with: tail -f $LOG_FILE"
        return 0
    else
        echo "❌ Failed to start daemon"
        rm -f "$PID_FILE"
        return 1
    fi
}
stop_daemon() {
    if [ ! -f "$PID_FILE" ]; then
        echo "LLMdiver daemon is not running"
        return 1
    fi
    PID=$(cat "$PID_FILE")
    echo "Stopping LLMdiver daemon (PID: $PID)..."
    if ps -p "$PID" > /dev/null 2>&1; then
        kill "$PID"
        sleep 2
        if ps -p "$PID" > /dev/null 2>&1; then
            echo "Force killing daemon..."
            kill -9 "$PID"
        fi
        rm -f "$PID_FILE"
        echo "✅ LLMdiver daemon stopped"
    else
        echo "Daemon was not running, removing PID file"
        rm -f "$PID_FILE"
    fi
}
status_daemon() {
    if [ -f "$PID_FILE" ]; then
        PID=$(cat "$PID_FILE")
        if ps -p "$PID" > /dev/null 2>&1; then
            echo "✅ LLMdiver daemon is running (PID: $PID)"
            if [ -f "$LOG_FILE" ]; then
                echo ""
                echo "📋 Recent log entries:"
                tail -n 5 "$LOG_FILE"
            fi
            if [ -d ".llmdiver" ]; then
                echo ""
                echo "📁 Recent analyses:"
                ls -lt .llmdiver/*.md 2>/dev/null | head -n 3
            fi
            return 0
        else
            echo "❌ LLMdiver daemon is not running (stale PID file)"
            return 1
        fi
    else
        echo "❌ LLMdiver daemon is not running"
        return 1
    fi
}
restart_daemon() {
    stop_daemon
    sleep 1
    start_daemon
}
show_logs() {
    if [ -f "$LOG_FILE" ]; then
        tail -f "$LOG_FILE"
    else
        echo "No log file found"
    fi
}
case "$1" in
    start)
        start_daemon
        ;;
    stop)
        stop_daemon
        ;;
    restart)
        restart_daemon
        ;;
    status)
        status_daemon
        ;;
    logs)
        show_logs
        ;;
    *)
        echo "Usage: $0 {start|stop|restart|status|logs}"
        echo ""
        echo "Commands:"
        echo "  start   - Start the LLMdiver daemon"
        echo "  stop    - Stop the LLMdiver daemon"
        echo "  restart - Restart the LLMdiver daemon"
        echo "  status  - Show daemon status and recent activity"
        echo "  logs    - Show live daemon logs"
        echo ""
        echo "Examples:"
        echo "  ./start_llmdiver.sh start"
        echo "  ./start_llmdiver.sh status"
        echo "  ./start_llmdiver.sh logs"
        exit 1
        ;;
esac
````

## File: start_with_audit.sh
````bash
TARGET_REPO="../GMAILspambot"
AUDIT_SCRIPT="./run_llm_audit.sh"
CLAUDE_CMD="claude --dangerously-skip-permissions --continue"
AUDIT_FILE="./audits/$(basename "$TARGET_REPO")/claude.md"
echo "🔍 Running preflight audit on $TARGET_REPO..."
$AUDIT_SCRIPT "$TARGET_REPO" --fast
echo "🚀 Starting Claude Code with audit context..."
$CLAUDE_CMD <<EOF
Please load the following audit report into memory and continuously reference it while working on this project.
The audit is in Markdown format and includes TODOs, mocks, dead code, and unwired modules. Periodically recheck the file to stay up-to-date.
--- START OF AUDIT ---
$(cat "$AUDIT_FILE")
--- END OF AUDIT ---
EOF
````

## File: start-llmdiver.sh
````bash
set -e
SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
LOG_DIR="$SCRIPT_DIR/logs"
METRICS_DIR="$SCRIPT_DIR/metrics"
PID_FILE="/tmp/llmdiver-daemon.pid"
LOG_FILE="$LOG_DIR/llmdiver.log"
METRICS_FILE="$METRICS_DIR/metrics.json"
mkdir -p "$LOG_DIR" "$METRICS_DIR"
if [[ -f "$LOG_FILE" && $(stat -f%z "$LOG_FILE" 2>/dev/null || stat -c%s "$LOG_FILE") -gt 104857600 ]]; then
    mv "$LOG_FILE" "$LOG_FILE.$(date +%Y%m%d-%H%M%S)"
    gzip "$LOG_FILE".* &
fi
check_health() {
    if ! curl -s http://localhost:8080/status >/dev/null; then
        echo "❌ Daemon is not responding"
        return 1
    fi
    echo "✅ Daemon is healthy"
    return 0
}
start_daemon() {
    echo "🚀 Starting LLMdiver daemon..."
    if [[ -f "$PID_FILE" ]]; then
        if kill -0 $(cat "$PID_FILE") 2>/dev/null; then
            echo "⚠️ Daemon is already running"
            return 1
        fi
        rm "$PID_FILE"
    fi
    python3 "$SCRIPT_DIR/llmdiver-daemon.py" > "$LOG_FILE" 2>&1 &
    echo $! > "$PID_FILE"
    sleep 2
    if ! check_health; then
        echo "❌ Failed to start daemon"
        return 1
    fi
    echo "✅ Daemon started successfully"
    return 0
}
stop_daemon() {
    echo "🛑 Stopping LLMdiver daemon..."
    if [[ ! -f "$PID_FILE" ]]; then
        echo "⚠️ No PID file found"
        return 0
    fi
    pid=$(cat "$PID_FILE")
    if ! kill -0 "$pid" 2>/dev/null; then
        echo "⚠️ Process not found"
        rm "$PID_FILE"
        return 0
    fi
    kill "$pid"
    rm "$PID_FILE"
    echo "✅ Daemon stopped"
    return 0
}
show_status() {
    if [[ ! -f "$PID_FILE" ]]; then
        echo "❌ Daemon is not running"
        return 1
    fi
    pid=$(cat "$PID_FILE")
    if ! kill -0 "$pid" 2>/dev/null; then
        echo "❌ Daemon process not found (PID: $pid)"
        rm "$PID_FILE"
        return 1
    fi
    if check_health; then
        if [[ -f "$METRICS_FILE" ]]; then
            echo -e "\n📊 Performance Metrics:"
            jq . "$METRICS_FILE"
        fi
        return 0
    fi
    return 1
}
case "${1:-status}" in
    start)
        start_daemon
        ;;
    stop)
        stop_daemon
        ;;
    restart)
        stop_daemon
        sleep 1
        start_daemon
        ;;
    status)
        show_status
        ;;
    health)
        check_health
        ;;
    *)
        echo "Usage: $0 {start|stop|restart|status|health}"
        exit 1
        ;;
esac
````

## File: test_llmdiver.sh
````bash
echo "=== LLMdiver Migration Test ==="
echo
echo "1. Testing configuration..."
cd /home/greenantix/AI/LLMdiver
python3 -c "
from llmdiver_daemon import LLMdiverConfig
config = LLMdiverConfig('config/llmdiver.json')
print(f'✅ Configuration loaded: {len(config.config[\"repositories\"])} repositories')
"
echo
echo "2. Testing multi-project discovery..."
python3 -c "
from llmdiver_daemon import MultiProjectManager
config = {'multi_project': {'enabled': True, 'projects_root': '/home/greenantix/AI', 'auto_discover': True, 'discovery_patterns': ['.git'], 'exclude_paths': ['node_modules', '__pycache__']}}
manager = MultiProjectManager(config)
projects = manager.discover_projects()
print(f'✅ Discovered {len(projects)} projects')
"
echo
echo "3. Testing manifest analysis..."
python3 -c "
from llmdiver_daemon import ManifestAnalyzer
config = {'manifest_analysis': {'enabled': True, 'manifest_files': ['package.json', 'requirements.txt']}}
analyzer = ManifestAnalyzer(config)
manifests = analyzer.find_manifests('/home/greenantix/AI/GMAILspambot')
print(f'✅ Found {len(manifests)} manifests')
"
echo
echo "4. Testing daemon initialization..."
python3 -c "
from llmdiver_daemon import LLMdiverDaemon
import logging
logging.getLogger().setLevel(logging.ERROR)  # Suppress info logs
daemon = LLMdiverDaemon()
print(f'✅ Daemon initialized with {len(daemon.config.config[\"repositories\"])} repositories')
"
echo
echo "5. Checking dependencies..."
if command -v repomix &> /dev/null; then
    echo "✅ repomix available"
else
    echo "❌ repomix not found"
fi
if python3 -c "import git, watchdog, requests" 2>/dev/null; then
    echo "✅ Python dependencies available"
else
    echo "❌ Missing Python dependencies"
fi
echo
echo "6. Testing LM Studio connectivity..."
if curl -s http://127.0.0.1:1234/v1/models >/dev/null 2>&1; then
    echo "✅ LM Studio accessible"
else
    echo "⚠️  LM Studio not accessible (this is optional)"
fi
echo
echo "=== Migration Test Complete ==="
echo "The LLMdiver daemon has been successfully migrated and configured!"
echo
echo "To start the daemon:"
echo "  ./start_llmdiver.sh start"
echo
echo "To check status:"
echo "  ./start_llmdiver.sh status"
echo
echo "To view logs:"
echo "  ./start_llmdiver.sh logs"
````

## File: watch_and_audit.sh
````bash
TARGET_REPO="../GMAILspambot"
AUDIT_SCRIPT="./run_llm_audit.sh"
find "$TARGET_REPO" -type f | entr -c "$AUDIT_SCRIPT" "$TARGET_REPO" --fast
````

## File: llmdiver_monitor.py
````python
class LLMdiverMonitor
⋮----
def __init__(self, root)
⋮----
style = ttk.Style()
⋮----
def setup_interface(self)
⋮----
notebook = ttk.Notebook(self.root)
⋮----
def setup_dashboard_tab(self)
⋮----
status_frame = ttk.LabelFrame(self.dashboard_frame, text="🔍 System Status", padding=10)
⋮----
control_frame = ttk.LabelFrame(self.dashboard_frame, text="🎮 Control Panel", padding=10)
⋮----
button_frame = ttk.Frame(control_frame)
⋮----
stats_frame = ttk.LabelFrame(self.dashboard_frame, text="📈 Quick Stats", padding=10)
⋮----
stats_columns = ('Metric', 'Value', 'Last Updated')
⋮----
stats_scrollbar = ttk.Scrollbar(stats_frame, orient=tk.VERTICAL, command=self.stats_tree.yview)
⋮----
def setup_logs_tab(self)
⋮----
log_control_frame = ttk.Frame(self.logs_frame)
⋮----
log_frame = ttk.LabelFrame(self.logs_frame, text="📝 Live Daemon Logs", padding=5)
⋮----
def setup_config_tab(self)
⋮----
config_control_frame = ttk.Frame(self.config_frame)
⋮----
config_editor_frame = ttk.LabelFrame(self.config_frame, text="⚙️ Configuration Editor", padding=5)
⋮----
def setup_results_tab(self)
⋮----
results_control_frame = ttk.Frame(self.results_frame)
⋮----
results_list_frame = ttk.LabelFrame(self.results_frame, text="📊 Recent Analysis Results", padding=5)
⋮----
results_columns = ('Project', 'Timestamp', 'Type', 'Status', 'Issues Found')
⋮----
results_scrollbar = ttk.Scrollbar(results_list_frame, orient=tk.VERTICAL, command=self.results_tree.yview)
⋮----
preview_frame = ttk.LabelFrame(self.results_frame, text="👁️ Preview", padding=5)
⋮----
def validate_configuration(self)
⋮----
config_data = self.config_text.get("1.0", tk.END)
⋮----
def setup_system_tab(self)
⋮----
system_info_frame = ttk.LabelFrame(self.system_frame, text="🖥️ System Information", padding=10)
⋮----
def start_daemon(self)
def stop_daemon(self)
def restart_daemon(self)
def force_analysis(self)
def run_command_in_thread(self, command: list, success_msg: str, failure_msg: str)
⋮----
def command_thread()
⋮----
result = subprocess.run(command, capture_output=True, text=True, timeout=300)
⋮----
error_details = result.stderr if result.stderr else "No error details available."
⋮----
thread = threading.Thread(target=command_thread, daemon=True)
⋮----
def process_command_queue(self)
⋮----
result = subprocess.run(["./start_llmdiver.sh", "stop"],
⋮----
result = subprocess.run(["./run_llm_audit.sh"], capture_output=True, text=True, timeout=300)
⋮----
def check_daemon_status(self)
⋮----
pid = f.read().strip()
⋮----
process = psutil.Process(int(pid))
⋮----
create_time = process.create_time()
uptime = time.time() - create_time
uptime_str = self.format_uptime(uptime)
⋮----
memory_mb = process.memory_info().rss / 1024 / 1024
⋮----
def format_uptime(self, seconds)
⋮----
hours = int(seconds // 3600)
minutes = int((seconds % 3600) // 60)
⋮----
def start_status_monitoring(self)
⋮----
def monitor()
monitor_thread = threading.Thread(target=monitor, daemon=True)
⋮----
def start_log_monitoring(self)
⋮----
def monitor_logs()
⋮----
log_file = "llmdiver_daemon.log"
⋮----
line = f.readline()
⋮----
log_thread = threading.Thread(target=monitor_logs, daemon=True)
⋮----
def process_log_queue(self)
⋮----
line = self.log_queue.get_nowait()
⋮----
def display_log_line(self, line)
⋮----
filter_level = self.log_filter.get()
⋮----
tag = "INFO"
⋮----
tag = "WARNING"
⋮----
tag = "ERROR"
⋮----
tag = "CRITICAL"
timestamp = datetime.now().strftime("%H:%M:%S")
formatted_line = f"[{timestamp}] {line}\n"
⋮----
lines = self.log_text.get("1.0", tk.END).split("\n")
⋮----
def log_message(self, message)
def refresh_logs(self)
⋮----
logs = f.read()
⋮----
def clear_logs(self)
def save_logs(self)
⋮----
filename = filedialog.asksaveasfilename(
⋮----
def load_configuration(self)
⋮----
config_data = f.read()
⋮----
def save_configuration(self)
def browse_config_file(self)
⋮----
filename = filedialog.askopenfilename(
⋮----
def refresh_results(self)
⋮----
analysis_files = sorted(Path(".").glob("**/analysis_data_*.json"), key=os.path.getctime, reverse=True)
⋮----
data = json.load(f)
metadata = data.get("metadata", {})
findings = data.get("ai_analysis", {}).get("structured_findings", {})
project_name = metadata.get("project_name", "Unknown")
timestamp_str = metadata.get("timestamp", "Unknown")
timestamp = datetime.fromisoformat(timestamp_str).strftime("%Y-%m-%d %H:%M:%S")
analysis_type = metadata.get("analysis_type", "general").capitalize()
crit_count = len(findings.get("critical_issues", []))
high_count = len(findings.get("high_priority", []))
med_count = len(findings.get("medium_priority", []))
status = f"Crit: {crit_count}, High: {high_count}, Med: {med_count}"
⋮----
def open_selected_result(self, event=None)
⋮----
selection = self.results_tree.selection()
⋮----
json_file_path = self.results_tree.item(selection[0], "id")
⋮----
self.results_preview.tag_configure("high", foreground="#E65100")  # Dark Orange
self.results_preview.tag_configure("medium", foreground="#FDD835")  # Yellow/Ochre
⋮----
metadata = data.get('metadata', {})
findings = data.get('ai_analysis', {}).get('structured_findings', {})
⋮----
items = findings.get(key, [])
⋮----
def open_results_folder(self)
def update_stats(self)
⋮----
stats = [
⋮----
def get_log_file_size(self)
⋮----
size = os.path.getsize("llmdiver_daemon.log")
⋮----
def count_analysis_files(self)
⋮----
count = len(list(Path(".").glob("**/*analysis*.md")))
⋮----
def get_last_analysis_time(self)
⋮----
analysis_files = list(Path(".").glob("**/*analysis*.md"))
⋮----
latest = max(analysis_files, key=os.path.getctime)
mtime = os.path.getctime(latest)
⋮----
def update_system_info(self)
⋮----
info = []
⋮----
config = json.load(f)
⋮----
memory = psutil.virtual_memory()
⋮----
disk = psutil.disk_usage('.')
⋮----
dependencies = [
⋮----
def main()
⋮----
root = tk.Tk()
app = LLMdiverMonitor(root)
def on_closing()
````

## File: llmdiver-daemon.py
````python
SENTENCE_TRANSFORMERS_AVAILABLE = True
⋮----
SENTENCE_TRANSFORMERS_AVAILABLE = False
⋮----
LLAMA_CPP_AVAILABLE = True
⋮----
LLAMA_CPP_AVAILABLE = False
⋮----
logger = logging.getLogger('llmdiver-daemon')
class Config
⋮----
def __init__(self)
def load_config(self)
class GitAutomation
⋮----
def __init__(self, config, repo_config)
def should_commit(self, changed_files)
⋮----
current = self.repo.active_branch.name
⋮----
def generate_commit_message(self, analysis_result)
⋮----
template = self.config['commit_message_template']
summary = "Updated code analysis"
details = "Changes detected by LLMdiver automated analysis"
⋮----
summary = analysis_result['summary']
details = analysis_result.get('details', '')
⋮----
def commit_and_push(self, changed_files, analysis_result)
⋮----
message = self.generate_commit_message(analysis_result)
⋮----
def update_documentation(self)
⋮----
docs_path = Path(self.repo_config['path']) / 'docs'
⋮----
index_path = docs_path / 'llmdiver_analysis.md'
⋮----
analysis_path = Path(self.repo_config['path']) / '.llmdiver/repomix.md'
⋮----
class CodePreprocessor
⋮----
def __init__(self, remove_comments=True, remove_whitespace=True)
def _extract_file_sections(self, content: str) -> List[Dict]
⋮----
files = []
current_file_path = None
current_language = 'unknown'
current_content_lines = []
in_code_block = False
⋮----
current_file_path = line.replace("## File: ", "").strip()
⋮----
current_language = self._detect_language(current_file_path)
⋮----
in_code_block = True
lang_spec = line.replace("```", "").strip()
⋮----
current_language = lang_spec
⋮----
def _detect_language(self, file_path: str) -> str
⋮----
ext_map = {
ext = Path(file_path).suffix.lower()
⋮----
def preprocess_repomix_output(self, content: str) -> Dict
⋮----
files = self._extract_file_sections(content)
⋮----
total_size = sum(f['size'] for f in files)
avg_size = total_size / len(files) if files else 0
metrics = {
⋮----
def format_for_llm(self, preprocessed_data: Dict) -> str
⋮----
output = []
metrics = preprocessed_data['metrics']
⋮----
class CodeIndexer
⋮----
def __init__(self, config)
def _initialize_embedding_backend(self)
⋮----
model_choice = self.config.get("embedding_model", "tfidf")
⋮----
model_name = self.config.get("model_name", "all-MiniLM-L6-v2")
⋮----
model_path = self.config.get("model_path", "")
⋮----
def find_similar_code(self, query_blocks, similarity_threshold=0.7)
⋮----
query_embeddings = self.embedding_backend.encode(query_blocks)
stored_embeddings = self.embedding_backend.encode(self.stored_blocks)
⋮----
query_embeddings = np.array([self.embedding_backend.embed(block) for block in query_blocks])
stored_embeddings = np.array([self.embedding_backend.embed(block) for block in self.stored_blocks])
⋮----
vectorizer = self.embedding_backend.fit(self.stored_blocks + query_blocks)
all_embeddings = vectorizer.transform(self.stored_blocks + query_blocks).toarray()
stored_embeddings = all_embeddings[:len(self.stored_blocks)]
query_embeddings = all_embeddings[len(self.stored_blocks):]
similar_blocks = []
⋮----
similarities = cosine_similarity([query_embedding], stored_embeddings)[0]
⋮----
seen = set()
unique_blocks = []
⋮----
block_key = (block['query_block'], block['similar_block'])
⋮----
def update_index(self, code_blocks)
class MetricsCollector
⋮----
def load_metrics(self)
def save_metrics(self)
def update_system_metrics(self)
⋮----
process = psutil.Process()
⋮----
def get_project_size(self) -> float
⋮----
total = 0
⋮----
def record_analysis(self, duration: float, token_count: int)
⋮----
stats = self.metrics['analysis']
⋮----
def record_api_call(self, endpoint: str, success: bool)
⋮----
stats = self.metrics['api']['requests'][endpoint]
⋮----
def record_git_operation(self, operation: str)
def get_metrics(self) -> Dict
class RepomixProcessor
⋮----
def create_preprocessor(self)
def create_intelligent_router(self)
def create_project_manager(self)
def run_repomix_analysis(self, repo_path)
⋮----
output_dir = Path(repo_path) / '.llmdiver'
⋮----
output_file = output_dir / 'repomix.md'
⋮----
cmd = ['repomix', repo_path, '--output', str(output_file)]
result = subprocess.run(cmd, check=True, capture_output=True, text=True)
⋮----
def _extract_structured_findings(self, analysis_text: str) -> Dict
⋮----
findings = {
section_map = {
current_section_key = None
⋮----
line_stripped = line.strip()
is_header = False
⋮----
header_text = line_stripped.replace('## ', '').lower()
⋮----
current_section_key = value
is_header = True
⋮----
# For lists, just append the content
⋮----
# For summary, append the whole line
⋮----
# Re-constitute the summary paragraph
⋮----
def analyze_manifest_changes(self, repo_config)
⋮----
repo_path = Path(repo_config['path'])
manifest_files = [
changes = []
⋮----
manifest_path = repo_path / manifest
⋮----
# Add manifest specific analysis here
⋮----
def enhanced_repository_analysis(self, repo_config: Dict)
⋮----
# Step 1: Run repomix
⋮----
summary = self.run_repomix_analysis(repo_config["path"])
⋮----
# Step 2: Preprocess and index code
⋮----
preprocessed_data = self.code_preprocessor.preprocess_repomix_output(summary)
⋮----
formatted_summary = self.code_preprocessor.format_for_llm(preprocessed_data)
⋮----
# Step 3: Get Semantic Context
⋮----
semantic_context = self.code_indexer.get_semantic_context(preprocessed_data)
⋮----
# Step 4: Analyze Dependencies
⋮----
manifest_analysis = self.analyze_manifest_changes(repo_config)
⋮----
# Step 5: Classify code and select prompt
⋮----
project_info = self.multi_project_manager.get_project_manifest_info(repo_config["path"])
analysis_type = self.intelligent_router.classify_code_changes(preprocessed_data)
⋮----
analysis_type = 'dependency'
⋮----
# Step 6: Construct the final prompt for the LLM
⋮----
enhanced_summary = f"""# Repository Analysis: {repo_config['name']}
⋮----
# Step 7: Send to LLM for analysis
⋮----
analysis = self.llm_client.analyze_repo_summary(enhanced_summary, analysis_type)
⋮----
# Step 8: Save results
⋮----
analysis_dir = Path(repo_config["path"]) / ".llmdiver"
⋮----
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
analysis_file = analysis_dir / f"enhanced_analysis_{timestamp}.md"
json_analysis_file = analysis_dir / f"analysis_data_{timestamp}.json"
analysis_data = {
⋮----
# Step 9: Git Automation
⋮----
commit_message = f"LLMdiver enhanced analysis for {repo_config['name']}"
⋮----
def run_analysis(self, repo_path)
⋮----
repomix_config = self.config['repomix']
⋮----
cmd = [
⋮----
start_time = time.time()
⋮----
duration = time.time() - start_time
⋮----
# Calculate token count
⋮----
encoder = tiktoken.get_encoding("cl100k_base")
token_count = len(encoder.encode(result.stdout))
⋮----
token_count = len(result.stdout) // 4  # rough estimate
# Record metrics
⋮----
# Handle git automation
⋮----
git_automation = self.git_automations[repo_config['name']]
changed_files = self.get_changed_files(repo_path)
⋮----
analysis_result = self.parse_analysis_result(output_file)
⋮----
self.metrics.record_analysis(time.time() - start_time, 0)  # failed run
def get_changed_files(self, repo_path)
⋮----
repo = git.Repo(repo_path)
⋮----
def parse_analysis_result(self, output_file)
⋮----
content = f.read()
# Simple parsing for now - could be enhanced
lines = content.split('\n')
⋮----
class FileChangeHandler(FileSystemEventHandler)
⋮----
def __init__(self, processor)
def on_modified(self, event)
⋮----
current_time = time.time()
⋮----
repo_path = os
````

## File: run_llm_audit.sh
````bash
set -e
if ! command -v jq &> /dev/null; then
    echo "❌ Error: jq is required but not installed"
    echo "Please install jq first:"
    echo "  Ubuntu/Debian: sudo apt-get install jq"
    echo "  MacOS: brew install jq"
    exit 1
fi
DRY_RUN=0
FAST_MODE=0
DEEP_MODE=0
SHOW_PAYLOAD=0
REPO_PATH=""
print_usage() {
    echo "Usage: $0 [OPTIONS] [path/to/repo]"
    echo
    echo "Options:"
    echo "  --dry           Dry run - only generate repo summary"
    echo "  --fast          Fast mode - minimal summary + quick LLM pass"
    echo "  --deep          Deep mode - full repo + architectural audit"
    echo "  --show-payload  Show LLM payload without running"
    echo "  -h, --help      Show this help message"
    echo
    echo "If no path is specified, the current directory will be used."
    exit 1
}
while [[ "$#" -gt 0 ]]; do
    case $1 in
        --dry) DRY_RUN=1 ;;
        --fast) FAST_MODE=1 ;;
        --deep) DEEP_MODE=1 ;;
        --show-payload) SHOW_PAYLOAD=1 ;;
        -h|--help) print_usage ;;
        --*) echo "❌ Unknown option: $1"; print_usage ;;
        *)
            if [[ -n "$REPO_PATH" ]]; then
                echo "❌ Multiple paths specified: $REPO_PATH and $1"
                print_usage
            fi
            REPO_PATH="$(realpath "$1")" ;;
    esac
    shift
done
# Fallback to current directory if no path specified
[[ -z "$REPO_PATH" ]] && REPO_PATH="$(pwd)"
if [[ ! -d "$REPO_PATH" ]]; then
    echo "❌ Error: Target directory not found: $REPO_PATH"
    echo "Usage: $0 [path/to/repo] [--dry] [--fast] [--deep]"
    exit 1
fi
PROJECT_NAME=$(basename "$REPO_PATH")
BASE_DIR="$(dirname "$REPO_PATH")"
AUDIT_DIR="$BASE_DIR/audits/$PROJECT_NAME"
TASKS_DIR="$AUDIT_DIR/tasks"
PROMPTS_DIR="$AUDIT_DIR/prompts"
LOGS_DIR="$AUDIT_DIR/logs"
TMP_DIR="$AUDIT_DIR/.tmp"
PROMPT_FILE="$PROMPTS_DIR/audit_plan.txt"
AUDIT_OUT="$AUDIT_DIR/full_audit.md"
PHASE4_LOG="$LOGS_DIR/${PROJECT_NAME}-phase4.md"
MIXED_FILE="$AUDIT_DIR/_repomix_summary.txt"
DEEP_AUDIT_FILE="$AUDIT_DIR/full_deep_audit.md"
CLAUDE_FILE="$AUDIT_DIR/claude.md"
for dir in "$AUDIT_DIR" "$TASKS_DIR" "$PROMPTS_DIR" "$LOGS_DIR" "$TMP_DIR"; do
    mkdir -p "$dir" || {
        echo "❌ Failed to create directory: $dir"
        exit 1
    }
done
rm -rf "$TMP_DIR"/*
export LLM_MODEL=${LLM_MODEL:-"meta-llama-3.1-8b-instruct"}
export LLM_TEMP=${LLM_TEMP:-0.3}
export LLM_URL=${LLM_URL:-"http://127.0.0.1:1234/v1/chat/completions"}
SUMMARY_FILE="$AUDIT_DIR/_repomix_summary.txt"
if [[ $DRY_RUN -eq 1 ]]; then
    echo "🔍 Dry run - will only generate repo summary"
elif [[ $FAST_MODE -eq 1 ]]; then
    echo "⚡ Fast mode enabled - minimal summary + fast LLM pass"
elif [[ $DEEP_MODE -eq 1 ]]; then
    echo "🧠 Deep mode enabled - full repo + architectural audit"
else
    echo "🔍 Standard audit - full summary + LLM analysis"
fi
if [[ -f "$REPO_PATH/.gitignore" ]]; then
    cp "$REPO_PATH/.gitignore" "$REPO_PATH/.gitignore.backup"
fi
cat > "$REPO_PATH/.gitignore.llmdiver" << EOF
node_modules/
dist/
build/
venv/
__pycache__/
.git/
site-packages/
*.log
*.tmp
EOF
echo "🔀 Generating repo mix with repomix..." > "$SUMMARY_FILE"
repomix "$REPO_PATH" \
  --output "$SUMMARY_FILE" \
  --style markdown \
  --compress \
  --remove-comments \
  --remove-empty-lines \
  --ignore "*.md" \
  --config-ignore ".gitignore.llmdiver" \
  --include "*.py,*.js,*.ts,*.jsx,*.tsx,*.sh" \
  --token-count-encoding cl100k_base
if [[ -f "$REPO_PATH/.gitignore.backup" ]]; then
    mv "$REPO_PATH/.gitignore.backup" "$REPO_PATH/.gitignore"
fi
echo "✅ Repomix summary created: $SUMMARY_FILE"
if [[ $DRY_RUN -eq 1 ]]; then
    exit 0
fi
if [[ $SHOW_PAYLOAD -eq 1 ]]; then
    echo "📋 Generating LM Studio payload preview..."
    generate_repomix
    user_prompt=$(cat "$MIXED_FILE" | jq -Rs .)
    system_prompt="You are a world-class code auditor. Analyze the following condensed repo summary and return issues grouped by TODOs, mocks/stubs, dead code, and unwired components."
    escaped_system_prompt=$(echo "$system_prompt" | jq -Rs .)
    echo "🔍 Payload that would be sent to LM Studio:"
    cat <<EOF | jq .
{
  "model": "$LLM_MODEL",
  "messages": [
    { "role": "system", "content": ${escaped_system_prompt} },
    { "role": "user", "content": ${user_prompt} }
  ],
  "temperature": $LLM_TEMP,
  "max_tokens": 4096,
  "stream": false
}
EOF
    exit 0
fi
function send_to_lm_studio() {
  local input_file="$1"
  local output_file="$2"
  local model="${LLM_MODEL:-meta-llama-3.1-8b-instruct}"
  local temperature="${LLM_TEMP:-0.3}"
  local lm_url="${LLM_URL:-http://127.0.0.1:1234/v1/chat/completions}"
  local system_prompt
  if [[ $DEEP_MODE -eq 1 ]]; then
    system_prompt="You are a principal software architect and security expert conducting a comprehensive deep architectural audit. This analysis will guide critical refactoring decisions and security improvements.
**DEEP ANALYSIS FRAMEWORK:**
Evaluate the codebase across these dimensions with severity-based prioritization:
**CRITICAL PRIORITY:**
- Security vulnerabilities and data exposure risks
- System stability threats and potential crashes
- Performance bottlenecks affecting user experience
**HIGH PRIORITY:**
- Architectural violations and design pattern misuse
- Missing error handling and edge cases
- Scalability limitations and technical debt
**MEDIUM PRIORITY:**
- Code maintainability and readability issues
- Test coverage gaps and quality concerns
- Documentation and API design problems
**REQUIRED OUTPUT FORMAT:**
## Executive Summary
[3-4 sentences: Overall architecture health, most critical risks, immediate action priorities]
## Critical Security & Stability Issues (CRITICAL)
[Security vulnerabilities, crash risks, data leaks - include file:line references and code snippets]
## Architectural Problems (HIGH)
[Design violations, coupling issues, pattern misuse - provide specific examples with locations]
## Performance & Scalability Concerns (HIGH)
[Bottlenecks, inefficient algorithms, resource usage - include measurements where possible]
## Technical Debt Analysis (MEDIUM)
[Maintainability issues, code smells, refactoring opportunities - prioritized by impact]
## Implementation Roadmap
[Phased approach: Quick wins (1-2 days), Major improvements (1-2 weeks), Strategic refactoring (1+ months)]
## Claude Code Action Items
[Specific, executable tasks for automated fixes with priority and estimated effort]
**ANALYSIS REQUIREMENTS:**
- Provide file:line references for all findings
- Include code snippets for context
- Assess business impact and technical risk
- Consider the project's domain and constraints
- Focus on actionable, measurable improvements"
  else
    system_prompt="You are a senior software engineer conducting a focused code audit for immediate improvements. Prioritize findings by impact and actionability.
**ANALYSIS PRIORITIES:**
- CRITICAL: Security issues, crash risks, data leaks
- HIGH: Performance problems, architectural violations
- MEDIUM: Technical debt, maintainability issues
- LOW: Style inconsistencies, minor optimizations
**REQUIRED OUTPUT FORMAT:**
## Executive Summary
[2-3 sentences: Key findings and recommended immediate actions]
## Critical Issues (CRITICAL/HIGH Priority)
[Security vulnerabilities, performance bottlenecks, architectural problems - with file:line references]
## Technical Debt & TODOs (MEDIUM Priority)
[TODO/FIXME items, mock implementations, maintainability issues - include locations]
## Dead Code & Optimizations (LOW Priority)
[Unused functions, redundant code, minor improvements - include removal suggestions]
## Quick Win Recommendations
[Top 3-5 actionable items that can be fixed immediately with high impact]
**REQUIREMENTS:**
- Provide specific file:line references for all findings
- Include relevant code snippets for context
- Focus on actionable recommendations, not theoretical issues"
  fi
  local user_prompt
  user_prompt=$(cat "$input_file" | jq -Rs .)
  local escaped_system_prompt
  escaped_system_prompt=$(echo "$system_prompt" | jq -Rs .)
  echo "📤 Sending repomix summary to LM Studio..."
  local payload
  payload=$(cat <<EOF
{
  "model": "$model",
  "messages": [
    { "role": "system", "content": ${escaped_system_prompt} },
    { "role": "user", "content": ${user_prompt} }
  ],
  "temperature": $temperature,
  "max_tokens": 4096,
  "stream": false
}
EOF
)
  local timeout=30
  [[ $DEEP_MODE -eq 1 ]] && timeout=120
  if ! curl -sf "$lm_url" >/dev/null 2>&1; then
    echo "❌ Error: LM Studio API not accessible at $lm_url"
    echo "Make sure LM Studio is running and API is enabled"
    exit 1
  fi
  response=$(curl -sS --max-time $timeout "$lm_url" \
    -H "Content-Type: application/json" \
    -d "$payload")
  if [[ $? -ne 0 ]]; then
    echo "❌ Error: Failed to connect to LM Studio"
    echo "Check if the service is running and accessible"
    exit 1
  fi
  if ! echo "$response" | jq -e '.choices[0].message.content' >/dev/null 2>&1; then
    echo "❌ Error: Invalid response from LM Studio"
    echo "Raw response: $response"
    exit 1
  fi
  echo "$response" | jq -r '.choices[0].message.content' > "$output_file"
  if [[ -s "$output_file" ]]; then
    if [[ $DEEP_MODE -eq 1 ]]; then
      echo "✅ Deep architectural audit complete: $output_file"
      echo "📄 To view the report, run: less $output_file"
    else
      echo "✅ LM Studio responded. Output saved to: $output_file"
    fi
  else
    echo "❌ LM Studio returned empty response. Check the summary or model settings."
  fi
}
echo "🔎 Extracting codebase signals..."
generate_repomix() {
    if [[ ! -f "$REPO_PATH/.gitignore.llmdiver" ]]; then
        cat > "$REPO_PATH/.gitignore.llmdiver" << EOF
node_modules/
dist/
build/
venv/
__pycache__/
.git/
site-packages/
*.log
*.tmp
EOF
    fi
    echo "🔀 Generating repo mix with repomix..." > "$MIXED_FILE"
    repomix "$REPO_PATH" \
        --output "$MIXED_FILE" \
        --style markdown \
        --compress \
        --remove-comments \
        --remove-empty-lines \
        --ignore "*.md" \
        --config-ignore ".gitignore.llmdiver" \
        --include "*.py,*.js,*.ts,*.jsx,*.tsx,*.sh" \
        --token-count-encoding cl100k_base
    local line_count=$(wc -l < "$MIXED_FILE")
    echo -e "\n✅ Repomix summary created ($line_count lines)\n" >> "$MIXED_FILE"
    if [[ $FAST_MODE -eq 1 ]]; then
        less "$MIXED_FILE"
    fi
}
generate_repomix
if [[ ! -f "$PROMPT_FILE" ]]; then
    echo "ℹ️ No audit plan found, using default..."
    mkdir -p "$PROMPTS_DIR" || {
        echo "❌ Failed to create prompts directory: $PROMPTS_DIR"
        exit 1
    }
    cat > "$PROMPT_FILE" << 'EOF'
You are an expert software auditor analyzing a codebase.
Focus on:
1. TODOs and tech debt that need addressing
2. Dead or duplicate code that can be removed
3. Mock/stub implementations that should be replaced
4. Unwired or orphaned components
5. Architectural improvements and refactoring opportunities
Format your response as markdown sections:
EOF
fi
START_TIME=$(date +%s)
echo "🤖 Running LM Studio analysis..."
if [[ $DEEP_MODE -eq 1 ]]; then
  echo "Using deep architectural analysis mode (timeout: 120s)..."
  send_to_lm_studio "$MIXED_FILE" "$DEEP_AUDIT_FILE"
else
  send_to_lm_studio "$MIXED_FILE" "$AUDIT_OUT"
fi
END_TIME=$(date +%s)
DURATION=$((END_TIME-START_TIME))
echo "📑 Splitting findings..."
split_section() {
    local section="$1"
    local file="$2"
    local header="$3"
    awk "/^## $section/{flag=1;next}/^## /{flag=0}flag" "$AUDIT_OUT" | \
        sed '/^\s*$/d' | \
        awk -v h="$header" 'BEGIN{print h "\n"} {print}' > "$file"
}
declare -A HEADERS
HEADERS["TODO Issues"]="---
status: pending
priority: high
assignee: claude_code
---"
HEADERS["Dead Code"]="---
status: pending
priority: medium
assignee: claude_code
---"
HEADERS["Mocks and Stubs"]="---
status: pending
priority: medium
assignee: claude_code
---"
HEADERS["Duplicate Code"]="---
status: pending
priority: low
assignee: claude_code
---"
HEADERS["Unwired Components"]="---
status: pending
priority: medium
assignee: claude_code
---"
for section in "TODO Issues" "Dead Code" "Mocks and Stubs" "Duplicate Code" "Unwired Components"; do
    outfile="$TASKS_DIR/${section,,}.md"
    outfile="${outfile// /_}"
    split_section "$section" "$outfile" "${HEADERS[$section]}"
done
echo "📝 Regenerating prompts..."
gen_claude_prompt() {
    local section="$1"
    local src="$2"
    local out="$3"
    local intro="$4"
    {
        echo "$intro"
        echo -e "\n## Audit Section: $section\n"
        awk 'f;/^---/{c++}c==2{f=1}' "$src" | sed '/^\s*$/d'
    } > "$out"
}
declare -A PROMPTS
PROMPTS["TODO Issues"]="You are Claude Code. Based on the findings below, perform the following:
- Fix the listed issues in-place
- Leave comments where changes are made
- Do not delete anything unless clearly marked dead/unreachable"
PROMPTS["Dead Code"]="You are Claude Code. Review the following dead code candidates and recommend safe removals or refactors. Only delete if clearly unreachable."
PROMPTS["Mocks and Stubs"]="You are Claude Code. Review the following mocks and stubs. Replace with real implementations where possible."
PROMPTS["Duplicate Code"]="You are Claude Code. Review the following duplicate code findings. Refactor to remove redundancy."
PROMPTS["Unwired Components"]="You are Claude Code. Review the following unwired components. Integrate or remove as appropriate."
for section in "TODO Issues" "Dead Code" "Mocks and Stubs" "Duplicate Code" "Unwired Components"; do
    infile="$TASKS_DIR/${section,,}.md"
    infile="${infile// /_}"
    outfile="$PROMPTS_DIR/CLAUDE_${section,,}.txt"
    outfile="${outfile// /_}"
    gen_claude_prompt "$section" "$infile" "$outfile" "${PROMPTS[$section]}"
done
mkdir -p "$(dirname "$PHASE4_LOG")" || {
    echo "❌ Failed to create log directory: $(dirname "$PHASE4_LOG")"
    exit 1
}
# Generate log with proper escaping
cat > "$PHASE4_LOG" << 'EOF'
- Time: $(date)
- Duration: ${DURATION}s
- Model: $LLM_MODEL
- Temperature: $LLM_TEMP
- Est. Input Tokens: ~$(wc -c < "$MIXED_FILE") chars/4
- Working Directory: $TMP_DIR
- $AUDIT_OUT (Full LM Studio analysis)
- $TASKS_DIR/todo_issues.md
- $TASKS_DIR/dead_code.md
- $TASKS_DIR/mocks_and_stubs.md
- $TASKS_DIR/duplicate_code.md
- $TASKS_DIR/unwired_components.md
- $PROMPTS_DIR/CLAUDE_todo_issues.txt
- $PROMPTS_DIR/CLAUDE_dead_code.txt
- $PROMPTS_DIR/CLAUDE_mocks_and_stubs.txt
- $PROMPTS_DIR/CLAUDE_duplicate_code.txt
- $PROMPTS_DIR/CLAUDE_unwired_components.txt
- $PHASE4_LOG (This summary)
\`\`\`sh
export LLM_MODEL="meta-llama-3.1-8b-instruct"
export LLM_TEMP=0.2
./run_llm_audit.sh
./run_llm_audit.sh --show-payload
./run_llm_audit.sh --dry
bash audits/show_task_status.sh
\`\`\`
1. Generated smart repo summary with repomix
   - Limited scan to high-signal patterns
   - Added file structure overview
   - Used intelligent output limits (50 lines per section)
2. Processed through LM Studio
   - Standard mode: Issues and tasks analysis
   - Deep mode: Full architectural assessment (120s timeout)
3. Split findings into task files
4. Generated task prompts
5. Created detailed summary reports
1. Review the audit in $AUDIT_OUT
2. Use \`send_to_claude.sh\` with generated prompts
3. Track progress in task files
EOF
if [[ -f "$AUDIT_OUT" ]]; then
    mkdir -p "$(dirname "$CLAUDE_FILE")" || {
        echo "❌ Failed to create directory for Claude tasks: $(dirname "$CLAUDE_FILE")"
        exit 1
    }
    # Generate with proper escaping and error checking
    {
        cat << EOF
---
project: ${PROJECT_NAME//\'/\'\\\'\'}
type: code-audit
status: pending
---
# Code Audit Tasks
EOF
        # Extract and transform sections, checking for errors
        if ! awk '/^## /{p=1}p' "$AUDIT_OUT" | sed 's/^##/###/'; then
            echo "❌ Failed to process audit sections"
            exit 1
        fi
    } > "$CLAUDE_FILE"
    if [[ -s "$CLAUDE_FILE" ]]; then
        echo "📋 Claude task list generated: $CLAUDE_FILE"
    else
        echo "⚠️ Warning: Generated Claude task list is empty"
    fi
fi
if [[ $DRY_RUN -eq 1 ]]; then
    echo "✨ Summary generated at: $MIXED_FILE"
    exit 0
fi
echo "✅ Audit complete for $PROJECT_NAME"
echo "📊 Summary: $PHASE4_LOG"
echo "📝 Claude tasks: $CLAUDE_FILE"
````

## File: llmdiver_daemon.py
````python
SENTENCE_TRANSFORMERS_AVAILABLE = True
⋮----
SENTENCE_TRANSFORMERS_AVAILABLE = False
⋮----
LLAMA_CPP_AVAILABLE = True
⋮----
LLAMA_CPP_AVAILABLE = False
class LLMdiverConfig
⋮----
def __init__(self, config_path: str = "config/llmdiver.json")
def load_config(self) -> Dict
⋮----
default_config = {
⋮----
config = json.load(f)
⋮----
class RepoWatcher(FileSystemEventHandler)
⋮----
def __init__(self, daemon, repo_config)
def on_modified(self, event)
⋮----
file_path = Path(event.src_path)
triggers = self.repo_config["analysis_triggers"]
⋮----
current_time = time.time()
⋮----
class LLMStudioClient
⋮----
def __init__(self, config)
def chunk_text(self, text: str, chunk_size: int = 16000) -> List[str]
⋮----
words = text.split()
chunks = []
current_chunk = []
current_length = 0
⋮----
word_length = len(word) + 1
⋮----
current_chunk = [word]
current_length = word_length
⋮----
def _load_specialized_prompts(self) -> Dict
def _detect_analysis_type(self, repo_config: Dict, manifest_analysis: str, file_changes: List[str] = None) -> str
⋮----
security_files = [f for f in file_changes if any(keyword in f.lower() for keyword in
ui_files = [f for f in file_changes if any(ext in f.lower() for ext in
config_files = [f for f in file_changes if any(keyword in f.lower() for keyword in
data_files = [f for f in file_changes if any(keyword in f.lower() for keyword in
⋮----
def analyze_repo_summary(self, summary_text: str, analysis_type: str = "general") -> str
⋮----
system_prompt = self.specialized_prompts.get(analysis_type, self.specialized_prompts["general"])
enable_chunking = self.config["llm_integration"].get("enable_chunking", False)
chunk_size = self.config["llm_integration"].get("chunk_size", 16000)
⋮----
chunks = self.chunk_text(summary_text, chunk_size)
all_analyses = []
⋮----
chunk_prompt = f"Analyze this repository section ({i+1}/{len(chunks)}):\n\n{chunk}"
analysis = self._send_single_request(system_prompt, chunk_prompt)
⋮----
combined = "\n\n".join(all_analyses)
final_prompt = f"Summarize and consolidate these code audit findings:\n\n{combined}"
⋮----
def _send_single_request(self, system_prompt: str, user_prompt: str) -> str
⋮----
payload = {
⋮----
response = requests.post(self.url, json=payload, timeout=300)
⋮----
result = response.json()
⋮----
error_text = ""
⋮----
error_data = e.response.json()
error_text = error_data.get("error", e.response.text)
⋮----
error_text = e.response.text
⋮----
class GitAutomation
⋮----
def analyze_changes(self, repo_path: str) -> Dict
⋮----
repo = git.Repo(repo_path)
modified_files = [item.a_path for item in repo.index.diff(None)]
untracked_files = repo.untracked_files
total_changes = len(modified_files) + len(untracked_files)
⋮----
def generate_commit_message(self, analysis: str, changes: Dict) -> str
⋮----
lines = analysis.split('\n')
critical_issues = []
todos = []
improvements = []
current_section = ""
⋮----
line = line.strip()
⋮----
current_section = "critical"
⋮----
current_section = "todos"
⋮----
current_section = "improvements"
⋮----
summary_parts = []
⋮----
summary = ", ".join(summary_parts) if summary_parts else "Update codebase"
details = []
⋮----
details_text = "\n".join(details)
⋮----
def auto_commit(self, repo_path: str, analysis: str) -> bool
⋮----
changes = self.analyze_changes(repo_path)
⋮----
repo = changes["repo"]
⋮----
message = self.generate_commit_message(analysis, changes)
⋮----
origin = repo.remote('origin')
⋮----
class ManifestAnalyzer
⋮----
def find_manifests(self, repo_path: str) -> List[str]
⋮----
manifests = []
repo_path = Path(repo_path)
⋮----
manifest_path = repo_path / manifest_file
⋮----
def analyze_manifest(self, manifest_path: str) -> Dict
⋮----
manifest_path = Path(manifest_path)
⋮----
content = f.read()
content_hash = hashlib.sha256(content.encode()).hexdigest()
analysis = {
⋮----
def _analyze_package_json(self, content: str) -> Dict
⋮----
data = json.loads(content)
dependencies = list(data.get("dependencies", {}).keys())
dev_dependencies = list(data.get("devDependencies", {}).keys())
⋮----
def _analyze_requirements_txt(self, content: str) -> Dict
⋮----
dependencies = []
⋮----
package = line.split('==')[0].split('>=')[0].split('<=')[0].split('~=')[0].strip()
⋮----
def _analyze_cargo_toml(self, content: str) -> Dict
⋮----
dev_dependencies = []
lines = content.split('\n')
current_section = None
⋮----
current_section = "dependencies"
⋮----
current_section = "dev_dependencies"
⋮----
package = line.split('=')[0].strip()
⋮----
def check_manifest_changes(self, repo_path: str) -> List[Dict]
⋮----
manifests = self.find_manifests(repo_path)
changes = []
⋮----
analysis = self.analyze_manifest(manifest_path)
⋮----
cache_key = manifest_path
⋮----
cached_analysis = self.manifests_cache[cache_key]
⋮----
class MultiProjectManager
⋮----
def discover_projects(self) -> List[Dict]
⋮----
projects = []
⋮----
pattern_path = item / pattern
⋮----
project_config = {
⋮----
def get_project_manifest_info(self, project_path: str) -> Dict
⋮----
manifest_analyzer = ManifestAnalyzer({"manifest_analysis": {"manifest_files": ["package.json", "requirements.txt", "Cargo.toml"]}})
manifests = manifest_analyzer.find_manifests(project_path)
info = {
⋮----
analysis = manifest_analyzer.analyze_manifest(manifest_path)
⋮----
class CodePreprocessor
⋮----
def __init__(self)
def preprocess_repomix_output(self, repomix_content: str) -> Dict
⋮----
structured_data = {
file_sections = self._extract_file_sections(repomix_content)
⋮----
processed_file = self._analyze_file_content(file_info)
⋮----
def _extract_file_sections(self, content: str) -> List[Dict]
⋮----
files = []
file_pattern = r'## File: (.+?)\n```(\w+)?\n(.*?)\n```'
matches = re.findall(file_pattern, content, re.DOTALL)
⋮----
def _detect_language(self, file_path: str) -> str
⋮----
ext_map = {
⋮----
def _analyze_file_content(self, file_info: Dict) -> Dict
⋮----
language = file_info['language']
content = file_info['content']
⋮----
patterns = self.language_patterns[language]
current_block = None
current_block_lines = []
⋮----
line_stripped = line.strip()
⋮----
block_type = self._categorize_line(line, patterns)
⋮----
current_block = block_type
current_block_lines = [line]
⋮----
def _categorize_line(self, line: str, patterns: Dict) -> str
def _generate_architecture_summary(self, files: List[Dict]) -> Dict
⋮----
summary = {
⋮----
lang = file_info['language']
⋮----
def _calculate_code_metrics(self, files: List[Dict]) -> Dict
⋮----
metrics = {
⋮----
def format_for_llm(self, structured_data: Dict) -> str
⋮----
output = []
arch = structured_data['architecture_summary']
⋮----
blocks = file_info['code_blocks']
⋮----
metrics = structured_data['code_metrics']
⋮----
class CodeIndexer
⋮----
def __init__(self, config: Dict, index_file: str = ".llmdiver/code_index.pkl")
def _initialize_embedding_backend(self)
⋮----
model_name = self.config.get("model_name", "all-MiniLM-L6-v2")
⋮----
model_path = self.config.get("model_path", "")
⋮----
def load_index(self)
⋮----
data = pickle.load(f)
⋮----
def save_index(self)
def extract_code_blocks(self, file_analysis: Dict) -> List[Dict]
⋮----
blocks = []
file_path = file_analysis['path']
language = file_analysis['language']
⋮----
def update_index(self, preprocessed_data: Dict)
⋮----
new_blocks = []
⋮----
blocks = self.extract_code_blocks(file_analysis)
⋮----
existing_files = {block['file'] for block in self.code_blocks}
updated_files = {block['file'] for block in new_blocks}
⋮----
def _rebuild_vectors(self)
⋮----
documents = []
⋮----
doc = f"{block['type']} {block['language']} {block['content']}"
⋮----
embeddings = []
⋮----
embedding = self.embedding_backend.create_embedding(doc)["data"][0]["embedding"]
⋮----
def find_similar_code(self, query_blocks: List[Dict], top_k: int = 5) -> List[Dict]
⋮----
similar_blocks = []
⋮----
query_doc = f"{query_block['type']} {query_block['language']} {query_block['content']}"
⋮----
query_vector = self.embedding_backend.encode([query_doc])
similarities = cosine_similarity(query_vector, self.vectors).flatten()
⋮----
query_embedding = self.embedding_backend.create_embedding(query_doc)["data"][0]["embedding"]
query_vector = np.array([query_embedding])
⋮----
query_vector = self.embedding_backend.transform([query_doc])
⋮----
top_indices = similarities.argsort()[-top_k-1:][::-1]
similarity_threshold = self.config.get("similarity_threshold", 0.1)
⋮----
similar_block = self.code_blocks[idx].copy()
⋮----
seen_files = set()
unique_blocks = []
⋮----
def get_semantic_context(self, current_changes: Dict) -> str
⋮----
query_blocks = []
⋮----
max_blocks = self.config.get("max_similar_blocks", 5)
similar_blocks = self.find_similar_code(query_blocks, top_k=max_blocks)
⋮----
context = "## Semantic Context (Similar Code Found Elsewhere)\n"
⋮----
class IntelligentRouter
⋮----
def __init__(self, llm_client)
⋮----
code_samples = []
file_extensions = []
⋮----
combined_code = '\n\n'.join(code_samples[:3])
⋮----
combined_code = combined_code[:2000] + "..."
classification_query = self.classification_prompt + combined_code
⋮----
classification = self._classify_with_llm(classification_query)
classification_mapping = {
result = classification_mapping.get(classification.lower(), 'general')
⋮----
def _classify_with_llm(self, prompt: str) -> str
⋮----
response = requests.post(self.llm_client.url, json=payload, timeout=30)
⋮----
classification = result["choices"][0]["message"]["content"].strip().lower()
valid_classifications = ['security', 'ui', 'data', 'config', 'api', 'testing', 'utility', 'business']
⋮----
def _classify_by_file_patterns(self, preprocessed_data: Dict) -> str
⋮----
file_paths = [f['path'] for f in preprocessed_data.get('files', [])]
security_count = sum(1 for path in file_paths if any(keyword in path.lower() for keyword in
ui_count = sum(1 for path in file_paths if any(ext in path.lower() for ext in
config_count = sum(1 for path in file_paths if any(keyword in path.lower() for keyword in
data_count = sum(1 for path in file_paths if any(keyword in path.lower() for keyword in
pattern_counts = {
max_pattern = max(pattern_counts.items(), key=lambda x: x[1])
⋮----
class LLMdiverDaemon
⋮----
def setup_logging(self)
⋮----
log_level = getattr(logging, self.config.config["daemon"]["log_level"])
⋮----
def run_repomix_analysis(self, repo_path: str) -> str
⋮----
output_file = f"{repo_path}/.llmdiver_analysis.md"
cmd = [
result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)
⋮----
def schedule_analysis(self, repo_config: Dict)
def process_analysis_queue(self)
⋮----
repo_config = self.analysis_queue.pop(0)
⋮----
def analyze_repository(self, repo_config: Dict)
def start_watching(self)
⋮----
repo_path = repo_config["path"]
⋮----
observer = Observer()
handler = RepoWatcher(self, repo_config)
⋮----
def start(self)
⋮----
analysis_thread = threading.Thread(target=self.process_analysis_queue)
⋮----
def discover_and_add_projects(self)
⋮----
discovered_projects = self.multi_project_manager.discover_projects()
existing_paths = {repo["path"] for repo in self.config.config["repositories"]}
⋮----
def analyze_manifest_changes(self, repo_config: Dict) -> str
⋮----
changes = self.manifest_analyzer.check_manifest_changes(repo_config["path"])
⋮----
analysis_text = "## Dependency Change Analysis\n\n"
⋮----
def _extract_structured_findings(self, analysis_text: str) -> Dict
⋮----
findings = {
lines = analysis_text.split('\n')
⋮----
current_content = []
⋮----
current_section = "executive_summary"
⋮----
current_section = "critical_issues"
⋮----
current_section = "high_priority"
⋮----
current_section = "medium_priority"
⋮----
current_section = "low_priority"
⋮----
current_section = "recommendations"
⋮----
def enhanced_repository_analysis(self, repo_config: Dict)
⋮----
summary = self.run_repomix_analysis(repo_config["path"])
⋮----
preprocessed_data = self.code_preprocessor.preprocess_repomix_output(summary)
formatted_summary = self.code_preprocessor.format_for_llm(preprocessed_data)
⋮----
semantic_context = self.code_indexer.get_semantic_context(preprocessed_data)
manifest_analysis = self.analyze_manifest_changes(repo_config)
project_info = self.multi_project_manager.get_project_manifest_info(repo_config["path"])
analysis_type = self.intelligent_router.classify_code_changes(preprocessed_data)
⋮----
analysis_type = 'dependency'
⋮----
enhanced_summary = f"""# Repository Analysis: {repo_config['name']}
analysis = self.llm_client.analyze_repo_summary(enhanced_summary, analysis_type)
analysis_dir = Path(repo_config["path"]) / ".llmdiver"
⋮----
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
analysis_file = analysis_dir / f"enhanced_analysis_{timestamp}.md"
json_analysis_file = analysis_dir / f"analysis_data_{timestamp}.json"
analysis_data = {
⋮----
latest_md_link = analysis_dir / "latest_enhanced_analysis.md"
latest_json_link = analysis_dir / "latest_analysis_data.json"
⋮----
commit_message = f"Enhanced analysis including manifest changes and project context"
enhanced_analysis_data = {
⋮----
def stop(self)
⋮----
"""Stop the daemon"""
⋮----
def signal_handler(signum, frame)
⋮----
"""Handle shutdown signals"""
⋮----
daemon = LLMdiverDaemon()
````
